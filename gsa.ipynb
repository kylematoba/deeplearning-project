{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gsa.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kylematoba/deeplearning-project/blob/master/gsa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "5ePor2KCrfQE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# How to get a free Tensor Processing Unit and use it to do interesting stuff\n"
      ]
    },
    {
      "metadata": {
        "id": "5uw41rc1pzdi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "  - Deep Learning: Statistical methods that achieve very expressive models of $y$ by composing several nonlinear functions of $x$. Generally characterised by requiring a lot of data and calculation.\n",
        "  - TensorFlow: Deep learning framework originally developed at Google Brain. Written in C++ and accessed most commonly (I think?) through Python.\n",
        "  - Keras: Very high level language for phrasing deep learning models, originally developed by a dude who subsequently went to Google. Most commonly (?) backed by TensforFlow, and in turn incorporated into TensorFlow.\n",
        "  - Tensor Processing Unit (TPU): A computer optimised for deep learning (relative to a GPU) by giving up unneeded stuff like high precision arithmetic and rasterisation.\n",
        "  - Colab: Google-hosted Jupyter notebooks that can be trivially changed to have a GPU or TPU backend.\n",
        "  \n",
        "  \n",
        "  \n",
        "  "
      ]
    },
    {
      "metadata": {
        "id": "NN_d1kv-s6u9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/shakespeare_with_tpu_and_keras.ipynb\n"
      ]
    },
    {
      "metadata": {
        "id": "5aJ8L1ONtYY2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " \n",
        "  - https://en.wikipedia.org/wiki/CIFAR-10#Research_Papers_Claiming_State-of-the-Art_Results_on_CIFAR-10\n",
        "    - State of the art entails many GPU-years to fit\n",
        "  - https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py\n",
        "  - https://dawn.cs.stanford.edu/benchmark/#cifar10\n",
        "  \n",
        "NB. TPU can be a bit touchy, for nontrivial models I recommend giving it a few seconds between compiling and starting to fit.\n",
        "    "
      ]
    },
    {
      "metadata": {
        "id": "9KObMONjtWol",
        "colab_type": "code",
        "outputId": "03034902-eea2-4e13-832a-7e64ef5773e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import keras\n",
        "import tensorflow\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "want_tpu = True\n",
        "use_tpu = want_tpu and ('COLAB_TPU_ADDR' in os.environ)\n",
        "batch_size = 32\n",
        "num_classes = 10\n",
        "epochs = 100\n",
        "\n",
        "optimizer = tensorflow.keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print('x_train shape: {}'.format(x_train.shape))\n",
        "print('training set size: {}'.format(x_train.shape[0]))\n",
        "print('test set size: {}'.format(x_test.shape[0]))\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "if use_tpu:\n",
        "    TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "    tpu_cluster_resolver = tensorflow.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)\n",
        "    using_single_core = False\n",
        "    strategy = tensorflow.contrib.tpu.TPUDistributionStrategy(tpu_cluster_resolver, using_single_core=using_single_core)\n",
        "    model = tensorflow.contrib.tpu.keras_to_tpu_model(model, strategy=strategy)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "training set size: 50000\n",
            "test set size: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Bv99Gye00rnW",
        "colab_type": "code",
        "outputId": "2d0d6926-1d05-4612-cf46-0a403b7fce39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "# time.sleep(1)\n",
        "data_augmentation = True\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        # randomly shift images horizontally (fraction of total width)\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically (fraction of total height)\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0.,  # set range for random shear\n",
        "        zoom_range=0.,  # set range for random zoom\n",
        "        channel_shift_range=0.,  # set range for random channel shifts\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        cval=0.,  # value used for fill_mode = \"constant\"\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False,  # randomly flip images\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        "\n",
        "    # Compute quantities required for feature-wise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # Fit the model on the batches generated by datagen.flow().\n",
        "    model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                        epochs=epochs,\n",
        "                        validation_data=(x_test, y_test))\n",
        "\n",
        "    \n",
        "# model.fit(x_train, y_train,\n",
        "#           batch_size=batch_size,\n",
        "#           epochs=epochs,\n",
        "#           validation_data=(x_test, y_test),\n",
        "#           shuffle=True)\n",
        "\n",
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using real-time data augmentation.\n",
            "Epoch 1/100\n",
            "1563/1563 [==============================] - 38s 25ms/step - loss: 1.8631 - acc: 0.3147 - val_loss: 1.5908 - val_acc: 0.4217\n",
            "Epoch 2/100\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 1.5797 - acc: 0.4217 - val_loss: 1.3737 - val_acc: 0.4960\n",
            "Epoch 3/100\n",
            "1400/1563 [=========================>....] - ETA: 3s - loss: 1.4700 - acc: 0.4675"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}