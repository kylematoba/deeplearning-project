\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
% \usepackage{bigfoot}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

\title{COMS W4995 Final Report: Automatic Data Augmentation Policy Selection}
\author{Jonathan D. Armstrong\\
{\tt\small jda2160@columbia.edu}
\and
Jesse Galef\\
{\tt\small jbg2160@columbia.edu}
\and
Kyle Matoba\\
{\tt\small km3227@columbia.edu}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
% \begin{abstract}
% \end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

In this project, we expanded upon the approach used in ``AutoAugment: Learning Augmentation Policies from Data'', \cite{Cubuk2018}. The essential problem here is to classify images, and the main innovation it builds upon is to augment the actual tagged training set with used to train a deep neural natwork with clever, automated transformations of the data that help to build a more robust and generliasable classifier. \autoref{fig:ship} demonstrates the idea: although the original image has been rotated and various areas randomly removed, a human would be able to tell that this picture depicts, and we wish for our deep learning model to be able to as well.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.35\textwidth]{ship.png}
\end{center}
\caption{An augmented CIFAR 10 image classified as a ship}
\label{fig:ship}
\end{figure}

This paper demonstrates success using reinforcement learning to search the parameter space of augmentation policies and identify high quality policies. When this data augmentation is combined with state of the art convolutional neural networks (CNNs), such as \cite{Yamada2018} it achieves state of the art results on the CIFAR 10 dataset,\footnote{\url{https://en.wikipedia.org/wiki/CIFAR-10\#Research\_Papers\_Claiming\_State-of-the-Art\_Results\_on\_CIFAR-10}} and several other canonical test problems in image classification.

We aimed to expand on the AutoAugment findings in two key ways:
By pre-selecting image transformations with simple selector, and
By learning a different data augmentation policy for each image class.

\section{Related works}

Data augmentation is indispensable in achieving state of the art performance in image classification. We have seen the best performance achieved by the best architecture wedded to the best data augmentation policy. One compelling finding to this effect comes from a paper that explores the possibility that models are overfitting CIFAR-10 data\cite{Recht2018} by testing against a newly collected CIFAR-10 test set, ``CIFAR-10.1''. Out of more than 20 models they entertained, the best-performing model on the CIFAR-10 dataset (\cite{Krizhevsky2009}) was a cutout (\cite{Devries2017}) regularised ``shake-shake'' architecture (\cite{Gastaldi2017}). Cutout is a data augmentation method which appends to the base data set additional occluded images that have had had contiguous regions set to ``zero'' (assuming the data has been normalised around this value).

Not only was the cutout regularized model best in test-set accuracy, but it was also best on the newly-collected ``CIFAR10.1'' dataset \cite{Recht2018} with the smallest drop in accuracy. The other well-performing cutout-regularised model, a wide resnet (\cite{Zagoruyko2016}), whilst beaten by some un-augmented models (though the shake-shake model itself has a straightforward interpretation as data augmentation applied to an representation), both in (test) and out of sample, sees a smaller dropoff between CIFAR 10 test and CIFAR10.1 data sets.

\section{The significant challenges in achieving the state of the art}

\cite{Cubuk2018} presents a fairly dramatic improvement to the state of the art on several standard benchmarks in image classification by fitting a very computationally demanding model many times to assess the criterion in a discrete optimisation problem. That is to say that this paper demands fairly considerable computation, and the authors, affiliated with Google Brain, clearly have it.

Using the author's source code, we were able to duplicate their results on CIFAR-10 with several of the smaller architectures, such as a 26 layer ``Shake-Shake'' model of dimension 32 (\cite{Gastaldi2017}) and a Wide Residual Network (\cite{Zagoruyko2016}) with depth 28 and widening factor 10.

For the latter, which required the least computation, a complete fit would have taken about two months on a relatively modern CPU. By running the calculation on the Nvidia K80 GPU offered by Google Colaboratory, we were able to bring the fitting time down to about 27 hours.

\subsection{Hardware Accelerators: GPUs and TPUs}
Getting the computation time down another order of magnitude would be ideal, as it would mean that quick experiments could be done inside of an hour, and the full run, delivering cutting edge results, could be done overnight.

This was in fact possible using TPUs, though unfortunately it was not possible to adapt the \cite{Cubuk2018} author's code to use TPUs, as it was written in an archaic flavour of Tensorflow, \cite{Abadi2016}. In fact, there is an overwhelming consensus online that using TPUs directly from Tensorflow is hazardous, and that using Keras' (\cite{Chollet2015}) \texttt{tf.contrib.tpu.keras\_to\_tpu\_model} method was the only mostly-certain way to avoid doing something very dumb. Happily, after much searching, we were able to find a cleanly-coded and correct implementation of the Wide ResNet in Keras. Happily as well, once we had a cleanly-running Keras model, it was straightforward to convert to running on a TPU.

\subsection{A significant aside: The model and success}
We elected to fit a relatively simpler and faster architecture, the Wide Residual Network (WRN or Wide ResNet, \cite{Zagoruyko2016}) -- our experiments showed that this model fit between 10 and 100 times more quickly than the best-performing models, so even on a TPU following best practice and under totally ideal conditions, it would not have been possible to do a fit of these models more than once or twice before the deadline. The author of \cite{Gastaldi2017} further bemoans the considerable computational challenges he faced in achieving his results as an individual without substantial computational resources.

		Since empirically, the only thing faster than a TPU would be a cluster of TPUs or a massive cluster of GPUs (we did not try the only other credible hardware-level acceleration we are aware of: 16 bit floats on chips having specialised support for them. Mostly this was because we found out about them only recently, but they seem to not be available via Google Cloud Services, and appear to require a good amount of tuning to get correct in any event), we can be confident that it would not have been possible to fit this model short of thousands of dollars of google cloud compute credit, or else some \emph{very} cutting-edge optimisations. In our researsh, most of the serious optimsations are coded in Pytorch (precluding the use of TPUs), and use (wide) resnets anyway. For one very cool flavour of this, see the ranking at \url{https://dawn.cs.stanford.edu/benchmark/#cifar10}) of the \cite{Coleman2017} DAWNBench project.

% Based on the comparison of autoaugmented and more simply augmented models presented in \cite{Cubuk2018}, we think that our results should transfer straightforwardly to architectures that perform better un-augmented. This is because the best results seem to be achieved pretty straightforwardly by stacking the best data augmentation schemes atop the best un-augmented architectures -- a separability essentially making totally state of the art results achievable to anyone with sufficient computational wherewithal to fit them.

% As mentioned, we achieve state of the art results in a particular format: the best classification performance from an off-the-shelf model that could be run for free by someone with a google account overnight.

\subsection{Google Colaboratory and Drive}
An embarassing quantity of effort went into adapting the Colaboratory platform for non-interactive use -- making it more robust to server-side outages and persisting data and results across sessions. This entailed a fairly deep delve into the programmatic control over Colaboratory and Jupyter Notebooks, as well as the better-documented Google Drive API (besides experiments with Dropbox and others). By the end, we had crafted a fairly sophisticated checkpointing and recovery process for the efficient and effective use of Google Colaboratory backed by Google Drive.

Whilst this time was essentially wasted in the sense that we learned nothing of academic or conceptual import, it was surely not wasted in a more general and perhaps more important issue: it let us achieve world class results without access to hundreds of petaflops of computation. This concern, that the best results may only in future be achievable to those with the financial resources, is a persistent gripe online (including influential movements such as ``fast.ai''\footnote{\url{https://www.fast.ai/about/}}), has featured in a recent Columbia Colloquim by John Henessy and has been a topic of considerable interest at the NeurIPS 2018 Conference. % and its attendant press coverage.\footnote{Cf. \url{https://www.bloomberg.com/news/articles/2018-11-30/visa-issues-cast-shadow-on-canada-s-moment-in-the-ai-spotlight}}

\section{Augmentation Policy Architecture}
We will use the following definitions (generalized from the AutoAugment Procedure):

\begin{enumerate}

\item[] \textbf{Transform}: a single transform that can be applied to an image. This transform is parameterized by the a level (e.g. magnitude of the transform, in the integer range $0,1 \cdots, 10$), and the probability of applying the transform. The probability is quantized, taking values in the range $\{0.0, 0.1, 0.2, \cdots, 0.9, 1.0\}$; the level is both quantized and normalized to a predefined range, for example the \textit{rotate} transform's maximum magnitude of $30$ degrees is the upper bound corresponding to a level of 10. Note that for many transforms such as \textit{auto\_contrast} and \textit{flip\_lr} ignore the level parameter.
\item[] \textbf{Sub-policy}: a sequence of transforms that are applied in series to an image
\item[] \textbf{Policy}: a set of Sub-Policies. For each mini-batch, a subpolicy is randomly chosen (uniformly) and applied.
\end{enumerate}

The set of twenty transforms considered exactly correspond to those of the AutoAugment transforms: \textit{flip\_lr}, \textit{flip\_ud}, \textit{auto\_contrast}, \textit{equalize}, \textit{invert}, \textit{rotate}, \textit{posterize}, \textit{crop}, \textit{solarize}, \textit{color}, \textit{contrast}, \textit{brightness}, \textit{sharpness}, \textit{shear\_x}, \textit{share\_y}, \textit{translate\_x}, \textit{translate\_y}, \textit{cutout}, \textit{blur}, and \textit{smooth}.

The AutoAugment policy architecture consists of five sub-policies, each built from two transforms. During training, for each mini-batch (number of images used for each step of gradient descent) a new sub-policy is uniformly selected at random and applied to all images within that mini-batch. On completion, a final super-policy is generated by concatenating sub-policies from the five best policies to create a policy with $25$ sub-policies. The authors used this process to generate policies from reduced versions of the CIFAR-10, SVHN, and ImageNet datasets.

For part of this paper, we consider a more generalized architecture that supports targeting transforms to individual image classes. In this scheme, a transform becomes an array of transforms with a correspondence between the array index and the coded image class. Further, we do not restrict ourselves to a fixed number of sub-policies nor a fixed number of transforms within a sub-policy.

\section{Results}

Note that in interpreting all of the following results (and analysis of the CIFAR data more generally), one should note that 100\% accuracy should seem to not even be achievable -- there are misclassifications in the raw data.\footnote{To see this, load the test data, e.g. with \texttt{\_, (xtest, ytest) = keras.datasets.cifar10.load\_data()}, then examine the 2406th entry, e.g. \texttt{plt.imshow(xtest[2405, :, :, :])} -- pretty evidently a frog, but labeled as a cat!}

\subsection{Class Based Policies}

In the original AutoAugment paper, each policy of data augmentation requires selecting 10 operations, each with one of 16 transformations, 11 discrete probabilities of application, and 10 magnitudes. The resulting action space has approximately $2.9 \times 10^{31}$ possible actions, making a policy-gradient reinforcement learning approach preferable. One challenge in vanilla policy gradient methods is finding a way to update weights without moving too far from the current policy. Several surrogate loss functions have been designed to encourage or enforce smaller steps, including Trust Region Policy Optimization (TRPO) and now Proximal Policy Optimization (PPO). AutoAugment uses a variant called PPO-Clip, developed by OpenAI \cite{Schulman2017}. PPO-Clip implements a surrogate loss function that includes a clipped term for how much more or less likely the sampled action trajectory would be under the new policy.We developed our own controller model which uses PPO-Clip to search through possible augmentation policy space, but with some reductions and additions:

\begin{itemize}
\item Each subpolicy is limited to a single transform with a random magnitude
\item The space of image transforms is reduced to those identified as promising in \autoref{sec:policies}.
\item Each image has different probabilities of using each of subpolicy when it is used in training, based on which imge class it is.Each policy output is a matrix in which each row corresponds to an image class and each column corresponds to a different transform.
\end{itemize}


\begin{figure}[h]
\begin{center}
\includegraphics[width=0.5\textwidth]{heatmap2.png}
\end{center}
\caption{Example policy for augmenting each image class differently, indicating the percent chance of selecting each transform for each class}
\label{fig:heatmap2}
\end{figure}

Each row - representing an image class - contains discrete values across the possible image transforms that were sampled from one of 10 parallel softmax layers, treating the probabilities as a multinomial distribution.

The example policy (\autoref{fig:heatmap2}) indicates that when being used in training, a bird image will have a 40\% chance of being solarized, but only a 10\% chance of being flipped left-right.

Since data augmentation which treats different image classes differently is relatively uncommon, the built-in functionality for the top models which user TPUs were unable to incorporate it. We are in the process of finding a way to apply our custom augmentation, with the backup of using less powerful models which will be slower to train.

\subsection{Na\"{\i}ve Analysis}
The results of this analysis strongly depend on the validity of comparing different test accuracies. Additionally, it is desirable to get meaningful results quickly since we will be retraining models to completion. In order to manage these requirement we used the following approaches

\begin{enumerate}
\item We removed dependency on the random initialization of weights by starting with a partially trained ``base'' model. Each policy is then trained using a copy of this base as a starting point. The number of epochs used is small to avoid getting trapped in a local minimum.
\item We must be sure each model has been fully trained. We acheive this (and reduced training time) by removing the model's dropout layers so that the validation loss achieves a prominent minimum. We then configure the training to hold the weights corresponding to the best validation accuracy. For this analysis, plots will be used to confirm training status.
\item To further reduce training time, we follow the lead of the AutoAgument authors and trained on a reduced version of the CIFAR-10 dataset consisting of 4000 images randomly selected from the training set.
\end{enumerate}

With the setup described above, we trained the base model against several different policies and achieved the results summarize in the table below.

\begin{table}[h]
\begin{tabular}{l|l}
\hline
Policy  			&Test Accuracy (\%)   \\ \hline
Baseline (No Augmentation)  	&49.6 \\
Single Transform		&46.6 to 53.1 \\
AutoAugment 			&53.3 \\ 
AutoAugment Random		&53.1 \\	
AutoAugment Subset Random 	&55.1 \\
Category Targeted Policies   	&50.0 to 54.7 \\
Best Transforms as Subpolicy 	&55.5 \\
\hline
\end{tabular}
\caption{Test accuracies with different policies}
\end{table}

\subsection{Policies}
\label{sec:policies}
To establish a \textbf{Baseline} policy, we first completed training with no data augmentation. Then, for each of the 20 transforms defined in \textit{AutoAugment}, we tested a policy built purely from that transform (fixed probability $0.5$ and level $5$); these are labeled collectively as the \textbf{Single Transform} policies. Many canonical transforms for CIFAR-10 made it to the top of the list including \textit{flip\_lr} (mirror) and \textit{rotate} with test accuracies of 53.1\% and 50.7\% respectively. On par with \textit{rotate} is also the less common transform \textit{solarize} with a test accuracy of 50.9\%. The worst performer here was \textit{blur} with a test accuracy of 46.6\%; considering the low resolution of the images this comes as no surprise.

In addition to testing the \textit{AutoAugment} policy, we developed two additional variants to explore the effectiveness of the AutoAugment procedure. First, \textbf{AutoAugment Random} is identical in structure to the original AutoAugment policy, but the transforms, probabilities, and levels are set randomly. Second, \textbf{AutoAugment Subset Random} also uses identical structure but draws from the subset of transforms that individually outperformed the \textbf{Baseline} policy by once percent (\textit{flip\_lr}, \textit{translate\_y}, \textit{translate\_x}, \textit{shear\_y}, \textit{solarize}, and \textit{rotate}). The most significant result here is that we were able to comfortably beat the performance of the \textbf{AutoAugment} policy by $1.7\%$ with a randomly generated policy informed by the results of our naive analysis.

Analysis of the category-level accuracies demonstrated significant differences between \textbf{Baseline} and \textbf{Single Transform} policies. For example, \textit{translate\_y} increased accuracy for the ``frog'' category by more than $37\%$. Using the test accuracy differences for each transform, we explored different algorithmic approaches to creating category targeted policies (with varying architecture) and achieved test accuracies from $50.0$ up to $54.7$. The architectures on the lower end of performance combined the best transforms for each category and mixed them into sub-policies; these policies topped out at $50.0\%$ test accuracy and were not able to outperform the best single transform (\textit{flip\_lr} at $53.1\%$). The architectures on the high end of performance use one transform for each sub-policy with identity masking for categories where the transform performed poorly. With this approach, we observed that test accuracy increased as we eased the identity masking threshold for each category. Ironically, this approach converged to the \textbf{Best Transforms as Subpolicy} policy which no longer differentiates between image categories! This policy was algorithmically generated by selecting the transforms that improved the baseline test accuracy by one percent and then used that transform as a sub-policy. This approach mimicked the \textit{AutoAugment Subset Random} policy in the transforms considered, but used a much simpler policy architecture with fixed levels and probability ($5$ and $0.5$ respectively). This policy obtained the best test accuracy, $55.5\%$.

		To further verify the effectiveness of the \textbf{Best Transforms as Subpolicy} policy, we retested with the full CIFAR-10 training dataset (with 10\% reserved for validation) and restored the dropout layers to the model. Note that unlike the previous results, each model was trained from scratch without using a shared partially-trained base model. The following table contains the results of this test.

\begin{table}[h]
\begin{tabular}{l|l}
\hline
Policy                          &Test Accuracy (\%)   \\ \hline
Baseline (No Augmentation)      &80.5 \\
AutoAugment                     &82.3 \\
Best Transforms as Subpolicy    &84.1 \\
\end{tabular}
\caption{Full dataset, model; test accuracies with different policies}
\end{table}

With \textbf{Best Transforms as Subpolicy} at the top again, these results provide evidence that it may be possible to outperform AutoAugment with a much simpler procedure. The success of the \textbf{Best Transforms as Subpolicy} architecture demonstrates the power of composition and its applicability to data augmentation; every transform that individually improves test accuracy is cast as a single-transform sub-policy and combined to create a new policy that outperforms any of its constituent transforms. Additionally note that this approach outperforms the original \textbf{AutoAugment} policy even though the transform parameter space is much less dynamic; probabilities are fixed at $0.5\%$ and the level are fixed at $5\%$.

\subsection{Wide ResNet model}

Here we compare the performance of our best policy, \textbf{Best Transforms as Subpolicy}, against \textbf{AutoAugment}.

\begin{table}[h]
\begin{tabular}{l|c|c}
\hline
Policy  &CIFAR-10 Test  &CIFAR-10.1 Test  \\
&Accuracy (\%)  &Accuracy (\%) \\ \hline
% AutoAugment      				&???	&??? \\
\cite{Cubuk2018} & 97.32 & Not reported \\
Best Transforms 			    &96.80	&92.40 \\
\, as Subpolicy					&		&	 \\
Cutout + Flipping & 96.10 & 91.80
\end{tabular}
\caption{WRN model \cite{Zagoruyko2016} test accuracies for CIFAR-10 and CIFAR-10.1}
\end{table}

% Note that the reported CIFAR-10 test accuracy for AutoAugment is $97.32\%$. The source of this difference is most likely a result of our transition to a TPU-based training setup. However, when using the same setups that only differ by data augmentation policies, our naive approach was able to perform similarly to the much more complicated AutoAugment procedure.

The reported accuracies for CIFAR-10 and CIFAR10.1 with the Wide ResNet policy are $97\%$ and $92\%$ respectively \cite{Recht2018}. This corresponds to a drop of five percent. In contrast our results show a significantly reduced drop in accuracy when evaluated on CIFAR-10.1; $drop_1\%$ for \textbf{AutoAument} and $4.4\%$ for \textbf{Best Transforms as Subpolicy}.

We like to think of these as near state of the art results in a particular format: the best classification performance from an off-the-shelf model that could be run for free by anyone without specialised hardware, overnight. This is a practically important and useful notion of goodness.\footnote{For example, see \url{https://www.technologyreview.com/s/612434/one-of-the-fathers-of-ai-is-worried-about-its-future/}.}

\subsection{Observations}

Another interesting phenomenon we noticed (we think!) with the Wide ResNet is the ``compression'' described by the professor in class, following \cite{Shwartz-Ziv2017}. The training and validation error tend to rise rapidly in only a few dozen epochs, but the generalisation error is terrible if one stops as soon as the errors begin flattening out (so bad, in fact, that we thought we had made a mistake). I found the mutual-information-based presentation interesting and subtle, but observing it in practice really made me realise how interesting and counterintuitive the result really is. I think that one of the most useful takeaways from this course is that the ``early stopping $\implies$ regularisation $\implies$ better generalisation error'' chain of implications is sloppy thinking and often incorrect in very relevant situations!

\section{Discussion}
Is the success of the AutoAugment procedure largely due to the general robustness of considering a large set of image transforms, many of which are not typically used? Our results at least indicate that an alternative search space may be more beneficial; perhaps exploring policy architectures could be more fruitful than exploring each transforms' parameterization (probability and level). Can a faster algorithm be developed that leverages composition?

Analysis of the published AutoAugment policies yields interesting results. For the CIFAR-10 policy, 7 of the 25 sub-policies effectively collapse to a single transform; this occurs when both transforms in a sub-policy are the same or when one of the transforms has its probability or level set to $0.0$. In contrast, this occurs for $1/25$ sub-policies of the SVHN dataset and for $4/25$ sub-policies of the ImageNet dataset. We suspect that the increased occurrence of this sub-policy structure are more common on CIFAR-10 due to the data inequality; since images are very low resolution (32 by 32 pixels) and have a large amount of variability within each category, the potential for information loss with every successive transform comes at the expense of accuracy. Alternatively, the lower occurrence of the transform collapse phenomena on the other datasets imply that our naive approach would need to be modified to consider sub-policies with more than one transform. 

% being presumptuous again 
Although we were not successful in demonstrating benefits of category-targeted transforms, the generalization should still be considered for other datasets. As is often done in mathematics, one can manufacture a representative example. Consider a dataset with two different image categories that express disjoint invariants; it is probably better to selectively apply transforms than to homogeneously apply the same transforms to all categories. This example is clearly artificial, but it is easy to imagine that as you increase the number and diversity of image categories within a dataset, you decrease the ability to identify transforms that improve the overall accuracy when applied to the entire dataset.

\section{Future work}

We are just beginning to obtain results, so in further work, we plan to do further experiments with different policy-finding algorithms. Most notably, we need to do a bit more development work to enable the subclass-specific augmentations, which cannot natively be represented in the preferred Keras \texttt{ImageDataGenerator} class.

\section{Link to Github}
%Currently the work is spread across a few repos, as well as Google Colab notebooks. We will work to consolidate things into a single, cleanly-runnable shape as we approach conclusion.

We worked hard to condense the state of the art run into a single file runnable by anyone with a google drive -- this is at: \url{https://github.com/kylematoba/deeplearning-project/blob/master/keras_wide_res_net_matoba.ipynb} (make sure to switch the backend to a TPU).

This report is available at \url{https://github.com/kylematoba/deeplearning-project/blob/master/final/jda2167_jbg2160_km3227.tex}

The background analysis used to find the policies are in \url{https://github.com/kylematoba/deeplearning-project/tree/master/code}.

% 	\begin{itemize}
% 	\item \url{https://github.com/kylematoba/deeplearning-project}
% 	\item \url{https://github.com/kylematoba/models}
% 	%\item %\url{https://colab.research.google.com/drive/1qV3vCsjnEcm5a8nRpN40n4qgVKKBzBkd#scrollTo=uiht7wpPPPCP} (also checked into the \texttt{deeplearning-project} repo above)
% 	%\item \url{https://colab.research.google.com/drive/13GpO9tSqLxflZ9Z2B6RkWMVzfcemW2v7}
% 	\end{itemize}

\nocite{Torralba2008}
{\small
\bibliographystyle{ieee}
\bibliography{../biblio}
}

\end{document}

